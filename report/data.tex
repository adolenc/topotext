\section{Data Used}
\label{sec:data_used}

\subsection{Description of data}
\label{sub:description_of_data}

Our data consisted of text documents in English language from 3 domains: news
articles about sports (\href{http://mlg.ucd.ie/datasets/bbc.html}{source}),
abstracts from scientific papers, and movie reviews. Per each domain we had 20
documents, which we later split on train and test sets with 10 documents per
set per domain. Each document contained at least 100 words in total, with
sports articles averaging exactly 150 words per document, scientific abstracts
$172.55$ words per document, and movie reviews $381.6$ words per document.

\subsection{Preparation of data}
\label{sub:preparation_of_data}

Since we wanted to build topological complexes on this data, we had to somehow
convert the textual documents into a set of points in some space of arbitrary
dimensions. A common way of doing this involves first preprocessing the words
in each text into their base form by applying word stemming and lemmatization
algorithms. We also removed all the stop words from the documents and ignored
whitespace in the resulting documents. After this process we are left with 60
documents (20 per domain), on which individually we then count various
occurrences:

\begin{itemize}
  \item average word length,
  \item average sentence length,
  \item shortest sentence length,
  \item total number of three most common words among all the words,
  \item number of words of length $\le 3$, and
  \item number of words of length $\ge 8$.
\end{itemize}

This gives us 6 counts per each document, which we can normalize to the
interval $[0, 1]$, and use as a point in $6$-dimensional space representing
the document.

We also used another popular method for text preprocessing called term
frequency-inverse document frequency (tf-idf for short)\footnote{You can read
more about tf-idf at \url{https://en.wikipedia.org/wiki/Tf-idf}.} to generate a
number of additional (more useful) features. The method tf-idf works as
follows. For each selected word $w$  we get one feature vector, meaning for
each text $t$ in the dataset we get one value, let's call it $v_{wt}$.  This
value is calculated by the formula $v_{wt} = tf(w, t)idf(w)$, where $tf(w, t)$
is the number of times a word $w$ appears in text $t$ and $idf(w) =
\log\frac{N}{d_{w}}$, where $N$ is the number of all texts in the dataset and
$d_{w}$ tells us in how many texts the word $w$ appears in. Intuitively this
means that words that appear in a lot of texts get weighed less (provide less
information). Typically we take $n$ most common words is the whole collection of
texts, which gives us $n$ features for each text ($n$ columns in a feature
matrix). While we played around with the number of features we got from tf-idf
(parameter $n$), this gave us a bunch of additional values per each document.
